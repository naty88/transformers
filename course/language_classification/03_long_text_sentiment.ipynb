{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Text Sentiment\n",
    "\n",
    "So far, we have restricted the length of the text being fed into our models. Bert in particular is restricted to consuming 512 tokens per sample. For many use-cases, this is most likely not a problem - but in some cases it can be.\n",
    "\n",
    "If we take the example of Reddit posts on the */r/investing* subreddit, many of the more important posts are **DD** (due-diligence), which often consists of deep dives into why the author thinks a stock is a good investment or not. On these longer pieces of text, the actual sentiment from the author may not be clear from the first 512 tokens. We need to consider the full post.\n",
    "\n",
    "Before working through the logic that allows us to consider the full post, let's import and define everything we need to make a prediction on a single chunk of text (using much of what we covered in the last section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# initialize our model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')\n",
    "\n",
    "# and we will place the processing of our input text into a function for easier prediction later\n",
    "def sentiment(tokens):\n",
    "    # get output logits from the model\n",
    "    output = model(**tokens)\n",
    "    # convert to probabilities\n",
    "    probs = torch.nn.functional.softmax(output[0], dim=-1)\n",
    "    # we will return the probability tensor (we will not need argmax until later)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to how we apply sentiment to longer pieces of text. There are two approaches that we cover in these notebooks:\n",
    "\n",
    "* Using neural text summarization to shorten the text to below 512 tokens.\n",
    "\n",
    "* Iterating through the text using a *window* and calculate the average article sentiment.\n",
    "\n",
    "In this notebook we will be using the second approach. The window in question will be a subsection of our tokenized text, of length `512`. First, let's define an example and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1345 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of your text: 6426\n",
      "Tokens: [1045, 2052, 2066, 2000, 2131, 2115, 2035, 4301, 2006, 1996, 5416, 10750, 3623, 2023, 2733, 1012, 1045, 2572, 2025, 5191, 2055, 1996, 3006, 2091, 22299, 2021, 1996, 5573, 3623, 1999, 16189, 1012, 2006, 1016, 1013, 2385, 1996, 2184, 2095, 9547, 16189, 3445, 2011, 2471, 1023, 3867, 1998, 2006, 1016, 1013, 2539, 1996, 10750, 3445, 2011, 2471, 1019, 3867, 1012, 3145, 2685, 2013, 1996, 27166, 9818, 3720, 1024, 1008, 1008, 1008, 1996, 1523, 6823, 2099, 9092, 24456, 1524, 1999, 2286, 2001, 1037, 5573, 9997, 1999, 9837, 16189, 2349, 2000, 3006, 6634, 2044, 1996, 2976, 3914, 2623, 2008, 2009, 2052, 4088, 6823, 4892, 2049, 20155, 24070, 2565, 1012, 1008, 1008, 1008, 1008, 1008, 2350, 2430, 5085, 2105, 1996, 2088, 2031, 3013, 3037, 6165, 2000, 3181, 2659, 2015, 1998, 3390, 15741, 12450, 1997, 11412, 17402, 1999, 1037, 7226, 2000, 5370, 2039, 1996, 4610, 2802, 1996, 6090, 3207, 7712, 1012, 1008, 1008, 1008, 1008, 1008, 2174, 1010, 1996, 3522, 4125, 1999, 16189, 6083, 2008, 2070, 9387, 2024, 3225, 2000, 3424, 6895, 17585, 1037, 18711, 1997, 3343, 10076, 2084, 11436, 2000, 8752, 1037, 4022, 4125, 1999, 14200, 1012, 1008, 1008, 1996, 3522, 4125, 1999, 5416, 16189, 1998, 1057, 1012, 1055, 1012, 14200, 10908, 2038, 2070, 9387, 15705, 2008, 1037, 9377, 1997, 1996, 2286, 1523, 6823, 2099, 9092, 24456, 1524, 2071, 2022, 2006, 1996, 9154, 1012, 1996, 6847, 10665, 1057, 1012, 1055, 1012, 2184, 1011, 2095, 9837, 3602, 6589, 2682, 1015, 1012, 1017, 1003, 2005, 1996, 2034, 2051, 2144, 2337, 12609, 3041, 2023, 2733, 1010, 2096, 1996, 2382, 1011, 2095, 5416, 2036, 2718, 2049, 3284, 2504, 2005, 1037, 2095, 1012, 16189, 2693, 19262, 2135, 2000, 5416, 7597, 1012, 16189, 7166, 2000, 4125, 1999, 11223, 2618, 2361, 2007, 14200, 10908, 1010, 2029, 2031, 2584, 2037, 3284, 3798, 1999, 1037, 5476, 1999, 1996, 1057, 1012, 1055, 1012, 1010, 6113, 2011, 3445, 16746, 1997, 1037, 2312, 10807, 19220, 7427, 1010, 5082, 2006, 17404, 4897, 12166, 1998, 7279, 2102, 1011, 2039, 7325, 5157, 1012, 1996, 1523, 6823, 2099, 9092, 24456, 1524, 1999, 2286, 2001, 1037, 5573, 9997, 1999, 9837, 16189, 2349, 2000, 3006, 6634, 2044, 1996, 2976, 3914, 2623, 2008, 2009, 2052, 4088, 6823, 4892, 2049, 20155, 24070, 2565, 1012, 2350, 2430, 5085, 2105, 1996, 2088, 2031, 3013, 3037, 6165, 2000, 3181, 2659, 2015, 1998, 3390, 15741, 12450, 1997, 11412, 17402, 1999, 1037, 7226, 2000, 5370, 2039, 1996, 4610, 2802, 1996, 6090, 3207, 7712, 1012, 1996, 7349, 1998, 2500, 2031, 5224, 16408, 12623, 1999, 3522, 3343, 6295, 1010, 19076, 2075, 2000, 2562, 3361, 3785, 6065, 2004, 1996, 3795, 4610, 3504, 2000, 12636, 2013, 1996, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 1012, 2174, 1010, 1996, 3522, 4125, 1999, 16189, 6083, 2008, 2070, 9387, 2024, 3225, 2000, 3424, 6895, 17585, 1037, 18711, 1997, 3343, 10076, 2084, 11436, 2000, 8752, 1037, 4022, 4125, 1999, 14200, 1012, 2007, 2430, 2924, 2490, 3718, 1010, 9547, 2788, 2991, 1999, 3976, 2029, 10255, 16189, 3020, 1012, 2023, 2064, 2036, 14437, 2058, 2046, 4518, 6089, 2004, 3020, 3037, 6165, 2965, 2062, 7016, 26804, 2005, 9786, 1010, 4786, 13066, 2000, 2128, 27241, 4757, 1996, 19920, 4044, 1012, 1523, 1996, 16408, 11032, 2013, 3343, 12088, 2097, 3497, 3961, 1999, 2173, 2127, 1996, 28896, 2031, 12308, 1037, 2126, 2000, 2070, 2709, 2000, 3671, 3012, 1010, 1524, 2056, 8683, 28352, 15256, 2213, 1010, 2708, 5211, 2961, 2012, 23622, 5211, 1010, 1999, 1037, 2470, 3602, 2023, 2733, 1012, 1523, 2174, 1010, 2045, 2097, 2022, 1037, 3891, 1997, 2178, 1520, 6823, 2099, 9092, 24456, 1521, 2714, 2000, 1996, 2028, 2057, 9741, 1999, 2286, 1010, 1998, 2023, 2003, 2256, 2364, 3579, 2005, 25682, 1010, 1524, 28352, 15256, 2213, 11310, 1010, 2323, 3343, 12088, 4088, 2000, 4895, 11101, 2023, 19220, 1012, 2146, 1011, 2744, 5416, 16189, 1999, 2900, 1998, 2885, 2628, 1057, 1012, 1055, 1012, 9837, 2015, 3020, 2646, 1996, 2203, 1997, 1996, 2733, 2004, 5416, 17794, 5429, 2037, 11103, 2015, 1012, 1523, 1996, 3571, 2003, 2008, 2122, 7045, 2024, 21125, 2000, 15401, 2043, 1996, 14925, 2497, 1998, 7349, 2453, 2776, 6823, 2099, 1010, 1524, 2056, 28328, 14891, 2100, 1010, 3026, 26632, 2358, 11657, 24063, 2012, 13926, 5243, 11412, 2968, 1010, 1999, 1037, 2470, 3602, 4709, 1523, 2210, 6823, 2099, 9092, 24456, 1012, 1524, 1523, 1996, 10238, 1997, 6823, 4892, 2024, 3271, 1999, 1996, 2142, 2163, 2011, 2488, 7027, 4341, 2044, 2176, 2706, 1997, 10520, 1998, 1996, 17626, 1997, 2312, 26354, 26620, 2013, 1996, 1002, 1015, 1012, 1023, 23458, 10807, 7427, 1012, 1524, 14891, 2100, 4081, 1996, 7349, 2052, 3497, 7949, 1996, 9367, 2006, 2049, 11412, 17402, 1010, 5549, 15172, 1996, 10745, 11071, 1999, 14200, 1012, 1523, 10067, 6089, 2031, 14831, 19762, 2000, 3020, 10750, 2004, 2009, 4107, 2019, 4522, 2000, 1996, 11443, 4859, 10750, 1998, 1037, 3020, 19575, 2000, 2146, 1011, 2744, 5356, 6223, 1010, 2437, 2068, 3579, 2062, 2006, 5396, 1011, 2744, 3930, 2107, 2004, 23750, 9777, 1524, 2002, 2056, 1012, 23750, 9777, 2024, 15768, 3005, 2836, 12102, 2000, 25705, 2007, 3171, 12709, 1012, 14891, 2100, 24273, 2023, 2832, 2000, 2022, 2062, 4417, 1999, 1996, 2117, 2431, 1997, 1996, 2095, 2043, 3171, 3930, 11214, 2039, 1010, 4852, 1996, 4022, 2005, 6823, 4892, 1012, 1001, 1001, 6823, 4892, 1999, 1996, 1057, 1012, 1055, 1012, 1010, 2021, 2025, 2885, 2035, 2937, 2480, 5766, 6291, 7151, 2063, 2409, 27166, 9818, 2006, 5958, 2008, 2045, 2001, 1037, 10056, 17856, 17905, 1999, 2129, 1996, 2446, 16021, 27595, 2003, 3241, 2055, 1996, 9824, 1997, 3037, 3446, 21857, 2015, 1012, 1523, 2028, 2003, 2885, 1010, 2073, 2057, 3613, 2000, 2031, 3361, 22422, 1010, 2073, 1996, 14925, 2497, 4247, 2000, 4965, 2039, 2000, 1996, 4098, 1999, 2344, 2000, 18478, 20861, 2090, 1996, 2167, 1998, 1996, 2148, 1517, 1996, 2844, 5703, 8697, 1998, 1996, 5410, 3924, 1517, 1998, 2012, 2070, 2391, 8307, 2097, 2031, 2000, 3477, 1996, 3976, 2005, 2008, 1010, 2021, 1999, 1996, 2460, 2744, 1045, 2123, 1521, 1056, 2156, 2151, 9997, 1999, 3037, 6165, 1010, 1524, 7151, 2063, 2056, 1010, 5815, 2008, 1996, 3663, 2003, 2367, 2163, 5178, 1012, 1523, 2138, 1997, 1996, 5294, 3454, 2008, 2031, 3047, 1010, 1996, 19220, 2008, 2003, 6230, 1010, 1996, 7922, 2108, 1996, 2088, 1521, 1055, 3914, 9598, 1010, 2045, 2003, 4415, 1037, 9874, 2000, 13299, 14200, 1998, 2009, 2003, 2183, 2000, 2272, 1012, 2153, 1010, 1045, 2123, 1521, 1056, 2113, 2043, 1998, 2129, 1010, 2021, 1996, 3037, 6165, 2031, 2042, 9561, 7406, 1998, 2027, 2323, 2022, 9561, 7406, 2582, 1012, 1524, 1001, 1001, 4803, 16189, 1037, 1520, 3671, 3444, 1521, 2174, 1010, 2025, 2035, 18288, 2024, 6427, 2008, 1996, 4125, 1999, 5416, 16189, 2003, 3430, 2005, 6089, 1012, 1999, 1037, 3602, 5958, 1010, 23724, 2015, 2132, 1997, 2647, 10067, 5656, 14459, 6187, 2226, 4081, 2008, 4803, 5416, 16189, 2020, 2058, 20041, 1010, 2004, 2027, 2018, 2042, 2474, 12588, 1996, 9229, 26632, 23035, 17680, 2005, 1996, 2117, 2431, 1997, 25682, 1010, 1998, 2056, 2027, 2020, 1037, 1523, 3671, 3444, 1524, 1997, 3171, 7233, 1012, 1523, 2007, 1996, 3145, 6853, 1997, 14200, 7302, 2039, 1010, 1996, 9824, 1997, 2130, 2062, 10807, 19220, 1999, 1996, 1057, 1012, 1055, 1012, 1998, 7279, 2102, 2039, 5157, 15801, 2011, 2152, 9987, 10995, 1010, 2009, 3849, 2157, 2005, 5416, 16189, 2000, 4608, 1011, 2039, 2007, 2060, 2062, 3935, 25416, 13490, 14279, 1010, 1524, 6187, 2226, 2056, 1010, 5815, 2008, 2430, 5085, 3961, 1523, 7933, 2006, 2907, 1524, 2445, 1996, 5703, 1997, 10831, 1012, 2002, 5275, 2008, 1996, 9561, 7406, 10750, 7774, 2003, 1523, 5171, 2012, 1996, 2220, 5711, 1997, 1996, 5402, 1010, 1524, 1998, 2008, 2061, 2146, 2004, 17404, 4897, 12166, 2024, 3144, 1010, 3930, 4247, 2000, 16356, 10745, 1998, 2430, 5085, 3961, 17145, 1010, 25416, 13490, 5649, 5829, 2408, 11412, 4280, 2298, 1523, 15123, 1524, 1998, 1041, 15549, 7368, 2323, 2022, 2583, 2000, 19319, 3020, 6165, 1012, 1523, 1997, 2607, 1010, 2044, 1996, 2844, 2693, 1997, 1996, 2197, 2261, 3134, 1010, 1041, 15549, 7368, 2071, 2928, 1037, 8724, 2004, 2116, 11105, 2008, 2031, 24356, 2007, 16189, 2298, 2058, 5092, 18533, 1010, 2066, 21955, 1998, 5085, 1010, 1524, 6187, 2226, 2056, 1012, 1523, 2021, 2012, 2023, 2754, 1010, 2057, 2228, 4803, 16189, 2024, 2062, 1037, 13964, 1997, 1996, 10067, 7087, 3006, 2084, 1037, 5081, 1010, 2061, 16510, 2015, 2323, 3613, 2000, 2022, 4149, 1012, 1524]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = \"\"\"\n",
    "I would like to get your all  thoughts on the bond yield increase this week.  I am not worried about the market downturn but the sudden increase in yields. On 2/16 the 10 year bonds yields increased by almost  9 percent and on 2/19 the yield increased by almost 5 percent.\n",
    "\n",
    "Key Points from the CNBC Article:\n",
    "\n",
    "* **The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.**\n",
    "* **Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic.**\n",
    "* **However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.**\n",
    "\n",
    "The recent rise in bond yields and U.S. inflation expectations has some investors wary that a repeat of the 2013 “taper tantrum” could be on the horizon.\n",
    "\n",
    "The benchmark U.S. 10-year Treasury note climbed above 1.3% for the first time since February 2020 earlier this week, while the 30-year bond also hit its highest level for a year. Yields move inversely to bond prices.\n",
    "\n",
    "Yields tend to rise in lockstep with inflation expectations, which have reached their highest levels in a decade in the U.S., powered by increased prospects of a large fiscal stimulus package, progress on vaccine rollouts and pent-up consumer demand.\n",
    "\n",
    "The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.\n",
    "\n",
    "Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic. The Fed and others have maintained supportive tones in recent policy meetings, vowing to keep financial conditions loose as the global economy looks to emerge from the Covid-19 pandemic.\n",
    "\n",
    "However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.\n",
    "\n",
    "With central bank support removed, bonds usually fall in price which sends yields higher. This can also spill over into stock markets as higher interest rates means more debt servicing for firms, causing traders to reassess the investing environment.\n",
    "\n",
    "“The supportive stance from policymakers will likely remain in place until the vaccines have paved a way to some return to normality,” said Shane Balkham, chief investment officer at Beaufort Investment, in a research note this week.\n",
    "\n",
    "“However, there will be a risk of another ‘taper tantrum’ similar to the one we witnessed in 2013, and this is our main focus for 2021,” Balkham projected, should policymakers begin to unwind this stimulus.\n",
    "\n",
    "Long-term bond yields in Japan and Europe followed U.S. Treasurys higher toward the end of the week as bondholders shifted their portfolios.\n",
    "\n",
    "“The fear is that these assets are priced to perfection when the ECB and Fed might eventually taper,” said Sebastien Galy, senior macro strategist at Nordea Asset Management, in a research note entitled “Little taper tantrum.”\n",
    "\n",
    "“The odds of tapering are helped in the United States by better retail sales after four months of disappointment and the expectation of large issuance from the $1.9 trillion fiscal package.”\n",
    "\n",
    "Galy suggested the Fed would likely extend the duration on its asset purchases, moderating the upward momentum in inflation.\n",
    "\n",
    "“Equity markets have reacted negatively to higher yield as it offers an alternative to the dividend yield and a higher discount to long-term cash flows, making them focus more on medium-term growth such as cyclicals” he said. Cyclicals are stocks whose performance tends to align with economic cycles.\n",
    "\n",
    "Galy expects this process to be more marked in the second half of the year when economic growth picks up, increasing the potential for tapering.\n",
    "\n",
    "## Tapering in the U.S., but not Europe\n",
    "\n",
    "Allianz CEO Oliver Bäte told CNBC on Friday that there was a geographical divergence in how the German insurer is thinking about the prospect of interest rate hikes.\n",
    "\n",
    "“One is Europe, where we continue to have financial repression, where the ECB continues to buy up to the max in order to minimize spreads between the north and the south — the strong balance sheets and the weak ones — and at some point somebody will have to pay the price for that, but in the short term I don’t see any spike in interest rates,” Bäte said, adding that the situation is different stateside.\n",
    "\n",
    "“Because of the massive programs that have happened, the stimulus that is happening, the dollar being the world’s reserve currency, there is clearly a trend to stoke inflation and it is going to come. Again, I don’t know when and how, but the interest rates have been steepening and they should be steepening further.”\n",
    "\n",
    "## Rising yields a ‘normal feature’\n",
    "\n",
    "However, not all analysts are convinced that the rise in bond yields is material for markets. In a note Friday, Barclays Head of European Equity Strategy Emmanuel Cau suggested that rising bond yields were overdue, as they had been lagging the improving macroeconomic outlook for the second half of 2021, and said they were a “normal feature” of economic recovery.\n",
    "\n",
    "“With the key drivers of inflation pointing up, the prospect of even more fiscal stimulus in the U.S. and pent up demand propelled by high excess savings, it seems right for bond yields to catch-up with other more advanced reflation trades,” Cau said, adding that central banks remain “firmly on hold” given the balance of risks.\n",
    "\n",
    "He argued that the steepening yield curve is “typical at the early stages of the cycle,” and that so long as vaccine rollouts are successful, growth continues to tick upward and central banks remain cautious, reflationary moves across asset classes look “justified” and equities should be able to withstand higher rates.\n",
    "\n",
    "“Of course, after the strong move of the last few weeks, equities could mark a pause as many sectors that have rallied with yields look overbought, like commodities and banks,” Cau said.\n",
    "\n",
    "“But at this stage, we think rising yields are more a confirmation of the equity bull market than a threat, so dips should continue to be bought.”\n",
    "\"\"\"\n",
    "print(f\"The size of your text: {len(txt)}\")\n",
    "tokens = tokenizer.encode_plus(txt, add_special_tokens=False)\n",
    "print(f\"Tokens: {tokens.input_ids}\")\n",
    "len(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ids: [1045, 2052, 2066, 2000, 2131, 2115, 2035, 4301]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'would', 'like', 'to', 'get', 'your', 'all', 'thoughts']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens_ids = tokens[\"input_ids\"][0:8]\n",
    "print(f\"Tokens ids: {test_tokens_ids}\")\n",
    "tokenizer.convert_ids_to_tokens(test_tokens_ids, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tokenize this longer piece of text we get a total of **1345** tokens, far too many to fit into our BERT model containing a maximum limit of 512 tokens. We will need to split this text into chunks of 512 tokens at a time, and calculate our sentiment probabilities for each chunk seperately.\n",
    "\n",
    "Because we are taking this slightly different approach, we have encoded our tokens using a different set of parameters to what we have used before. This time, we:\n",
    "\n",
    "* Avoided adding special tokens `add_special_tokens=False` because this will add *[CLS]* and *[SEP]* tokens to the start and end of the full tokenized tensor of length **1345**, we will instead add them manually later.\n",
    "\n",
    "* We will not specify `max_length`, `truncation`, or `padding` parameters (as we do not use any of them here).\n",
    "\n",
    "* We will return standard Python *lists* rather than tensors by not specifying `return_tensors` (it will return lists by default). This will make the following logic steps easier to follow - but we will rewrite them using PyTorch code in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we break our tokenized dictionary into `input_ids` and `attention_mask` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = tokens['input_ids']\n",
    "attention_mask = tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access slices of these lists like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1045,\n",
       " 2572,\n",
       " 2025,\n",
       " 5191,\n",
       " 2055,\n",
       " 1996,\n",
       " 3006,\n",
       " 2091,\n",
       " 22299,\n",
       " 2021,\n",
       " 1996,\n",
       " 5573,\n",
       " 3623,\n",
       " 1999,\n",
       " 16189,\n",
       " 1012]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[16:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using this to break our lists into smaller sections, let's test it in a simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=0\n",
      "end=512\n",
      "start=512\n",
      "end=1024\n",
      "start=1024\n",
      "end=1345\n"
     ]
    }
   ],
   "source": [
    "# define our starting position (0) and window size (number of tokens in each chunk)\n",
    "start = 0\n",
    "window_size = 512\n",
    "\n",
    "# get the total length of our tokens\n",
    "total_len = len(input_ids)\n",
    "\n",
    "# initialize condition for our while loop to run\n",
    "loop = True\n",
    "\n",
    "# loop through and print out start/end positions\n",
    "while loop:\n",
    "    # the end position is simply the start + window_size\n",
    "    end = start + window_size\n",
    "    # if the end position is greater than the total length, make this our final iteration\n",
    "    if end >= total_len:\n",
    "        loop = False\n",
    "        # and change our endpoint to the final token position\n",
    "        end = total_len\n",
    "    print(f\"{start=}\\n{end=}\")\n",
    "    # we need to move the window to the next 512 tokens\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logic works for shifting our window across the full length of input IDs, so now we can modify it to iterately predict sentiment for each window. There will be a few added steps for us to get this to work:\n",
    "\n",
    "1. Extract the window from `input_ids` and `attention_mask`.\n",
    "\n",
    "2. Add the start of sequence token `[CLS]`/`101` and seperator token `[SEP]`/`102`.\n",
    "\n",
    "3. Add padding (only applicable to final batch).\n",
    "\n",
    "4. Format into dictionary containing PyTorch tensors.\n",
    "\n",
    "5. Make logits predictions with the model.\n",
    "\n",
    "6. Calculate softmax and append softmax vector to a list `probs_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids_chunk: [101, 1045, 2052, 2066, 2000, 2131, 2115, 2035, 4301, 2006, 1996, 5416, 10750, 3623, 2023, 2733, 1012, 1045, 2572, 2025, 5191, 2055, 1996, 3006, 2091, 22299, 2021, 1996, 5573, 3623, 1999, 16189, 1012, 2006, 1016, 1013, 2385, 1996, 2184, 2095, 9547, 16189, 3445, 2011, 2471, 1023, 3867, 1998, 2006, 1016, 1013, 2539, 1996, 10750, 3445, 2011, 2471, 1019, 3867, 1012, 3145, 2685, 2013, 1996, 27166, 9818, 3720, 1024, 1008, 1008, 1008, 1996, 1523, 6823, 2099, 9092, 24456, 1524, 1999, 2286, 2001, 1037, 5573, 9997, 1999, 9837, 16189, 2349, 2000, 3006, 6634, 2044, 1996, 2976, 3914, 2623, 2008, 2009, 2052, 4088, 6823, 4892, 2049, 20155, 24070, 2565, 1012, 1008, 1008, 1008, 1008, 1008, 2350, 2430, 5085, 2105, 1996, 2088, 2031, 3013, 3037, 6165, 2000, 3181, 2659, 2015, 1998, 3390, 15741, 12450, 1997, 11412, 17402, 1999, 1037, 7226, 2000, 5370, 2039, 1996, 4610, 2802, 1996, 6090, 3207, 7712, 1012, 1008, 1008, 1008, 1008, 1008, 2174, 1010, 1996, 3522, 4125, 1999, 16189, 6083, 2008, 2070, 9387, 2024, 3225, 2000, 3424, 6895, 17585, 1037, 18711, 1997, 3343, 10076, 2084, 11436, 2000, 8752, 1037, 4022, 4125, 1999, 14200, 1012, 1008, 1008, 1996, 3522, 4125, 1999, 5416, 16189, 1998, 1057, 1012, 1055, 1012, 14200, 10908, 2038, 2070, 9387, 15705, 2008, 1037, 9377, 1997, 1996, 2286, 1523, 6823, 2099, 9092, 24456, 1524, 2071, 2022, 2006, 1996, 9154, 1012, 1996, 6847, 10665, 1057, 1012, 1055, 1012, 2184, 1011, 2095, 9837, 3602, 6589, 2682, 1015, 1012, 1017, 1003, 2005, 1996, 2034, 2051, 2144, 2337, 12609, 3041, 2023, 2733, 1010, 2096, 1996, 2382, 1011, 2095, 5416, 2036, 2718, 2049, 3284, 2504, 2005, 1037, 2095, 1012, 16189, 2693, 19262, 2135, 2000, 5416, 7597, 1012, 16189, 7166, 2000, 4125, 1999, 11223, 2618, 2361, 2007, 14200, 10908, 1010, 2029, 2031, 2584, 2037, 3284, 3798, 1999, 1037, 5476, 1999, 1996, 1057, 1012, 1055, 1012, 1010, 6113, 2011, 3445, 16746, 1997, 1037, 2312, 10807, 19220, 7427, 1010, 5082, 2006, 17404, 4897, 12166, 1998, 7279, 2102, 1011, 2039, 7325, 5157, 1012, 1996, 1523, 6823, 2099, 9092, 24456, 1524, 1999, 2286, 2001, 1037, 5573, 9997, 1999, 9837, 16189, 2349, 2000, 3006, 6634, 2044, 1996, 2976, 3914, 2623, 2008, 2009, 2052, 4088, 6823, 4892, 2049, 20155, 24070, 2565, 1012, 2350, 2430, 5085, 2105, 1996, 2088, 2031, 3013, 3037, 6165, 2000, 3181, 2659, 2015, 1998, 3390, 15741, 12450, 1997, 11412, 17402, 1999, 1037, 7226, 2000, 5370, 2039, 1996, 4610, 2802, 1996, 6090, 3207, 7712, 1012, 1996, 7349, 1998, 2500, 2031, 5224, 16408, 12623, 1999, 3522, 3343, 6295, 1010, 19076, 2075, 2000, 2562, 3361, 3785, 6065, 2004, 1996, 3795, 4610, 3504, 2000, 12636, 2013, 1996, 2522, 17258, 1011, 2539, 6090, 3207, 7712, 1012, 2174, 1010, 1996, 3522, 4125, 1999, 16189, 6083, 2008, 2070, 9387, 2024, 3225, 2000, 3424, 6895, 17585, 1037, 18711, 1997, 3343, 10076, 2084, 11436, 2000, 8752, 1037, 4022, 4125, 1999, 14200, 1012, 2007, 2430, 2924, 2490, 3718, 1010, 9547, 2788, 2991, 1999, 3976, 2029, 10255, 16189, 3020, 1012, 2023, 2064, 2036, 14437, 2058, 2046, 4518, 6089, 2004, 3020, 3037, 6165, 2965, 2062, 7016, 26804, 2005, 9786, 1010, 4786, 13066, 2000, 2128, 27241, 4757, 1996, 19920, 4044, 1012, 1523, 102]\n",
      "probs: tensor([[0.1384, 0.8145, 0.0471]], grad_fn=<SoftmaxBackward0>)\n",
      "pred ---  indices of the maximum value: 1\n",
      "input_ids_chunk: [101, 1996, 16408, 11032, 2013, 3343, 12088, 2097, 3497, 3961, 1999, 2173, 2127, 1996, 28896, 2031, 12308, 1037, 2126, 2000, 2070, 2709, 2000, 3671, 3012, 1010, 1524, 2056, 8683, 28352, 15256, 2213, 1010, 2708, 5211, 2961, 2012, 23622, 5211, 1010, 1999, 1037, 2470, 3602, 2023, 2733, 1012, 1523, 2174, 1010, 2045, 2097, 2022, 1037, 3891, 1997, 2178, 1520, 6823, 2099, 9092, 24456, 1521, 2714, 2000, 1996, 2028, 2057, 9741, 1999, 2286, 1010, 1998, 2023, 2003, 2256, 2364, 3579, 2005, 25682, 1010, 1524, 28352, 15256, 2213, 11310, 1010, 2323, 3343, 12088, 4088, 2000, 4895, 11101, 2023, 19220, 1012, 2146, 1011, 2744, 5416, 16189, 1999, 2900, 1998, 2885, 2628, 1057, 1012, 1055, 1012, 9837, 2015, 3020, 2646, 1996, 2203, 1997, 1996, 2733, 2004, 5416, 17794, 5429, 2037, 11103, 2015, 1012, 1523, 1996, 3571, 2003, 2008, 2122, 7045, 2024, 21125, 2000, 15401, 2043, 1996, 14925, 2497, 1998, 7349, 2453, 2776, 6823, 2099, 1010, 1524, 2056, 28328, 14891, 2100, 1010, 3026, 26632, 2358, 11657, 24063, 2012, 13926, 5243, 11412, 2968, 1010, 1999, 1037, 2470, 3602, 4709, 1523, 2210, 6823, 2099, 9092, 24456, 1012, 1524, 1523, 1996, 10238, 1997, 6823, 4892, 2024, 3271, 1999, 1996, 2142, 2163, 2011, 2488, 7027, 4341, 2044, 2176, 2706, 1997, 10520, 1998, 1996, 17626, 1997, 2312, 26354, 26620, 2013, 1996, 1002, 1015, 1012, 1023, 23458, 10807, 7427, 1012, 1524, 14891, 2100, 4081, 1996, 7349, 2052, 3497, 7949, 1996, 9367, 2006, 2049, 11412, 17402, 1010, 5549, 15172, 1996, 10745, 11071, 1999, 14200, 1012, 1523, 10067, 6089, 2031, 14831, 19762, 2000, 3020, 10750, 2004, 2009, 4107, 2019, 4522, 2000, 1996, 11443, 4859, 10750, 1998, 1037, 3020, 19575, 2000, 2146, 1011, 2744, 5356, 6223, 1010, 2437, 2068, 3579, 2062, 2006, 5396, 1011, 2744, 3930, 2107, 2004, 23750, 9777, 1524, 2002, 2056, 1012, 23750, 9777, 2024, 15768, 3005, 2836, 12102, 2000, 25705, 2007, 3171, 12709, 1012, 14891, 2100, 24273, 2023, 2832, 2000, 2022, 2062, 4417, 1999, 1996, 2117, 2431, 1997, 1996, 2095, 2043, 3171, 3930, 11214, 2039, 1010, 4852, 1996, 4022, 2005, 6823, 4892, 1012, 1001, 1001, 6823, 4892, 1999, 1996, 1057, 1012, 1055, 1012, 1010, 2021, 2025, 2885, 2035, 2937, 2480, 5766, 6291, 7151, 2063, 2409, 27166, 9818, 2006, 5958, 2008, 2045, 2001, 1037, 10056, 17856, 17905, 1999, 2129, 1996, 2446, 16021, 27595, 2003, 3241, 2055, 1996, 9824, 1997, 3037, 3446, 21857, 2015, 1012, 1523, 2028, 2003, 2885, 1010, 2073, 2057, 3613, 2000, 2031, 3361, 22422, 1010, 2073, 1996, 14925, 2497, 4247, 2000, 4965, 2039, 2000, 1996, 4098, 1999, 2344, 2000, 18478, 20861, 2090, 1996, 2167, 1998, 1996, 2148, 1517, 1996, 2844, 5703, 8697, 1998, 1996, 5410, 3924, 1517, 1998, 2012, 2070, 2391, 8307, 2097, 2031, 2000, 3477, 1996, 3976, 2005, 2008, 1010, 2021, 1999, 1996, 2460, 2744, 1045, 2123, 1521, 1056, 2156, 2151, 9997, 1999, 3037, 6165, 1010, 1524, 7151, 2063, 2056, 1010, 5815, 2008, 1996, 3663, 2003, 2367, 2163, 5178, 1012, 1523, 2138, 1997, 1996, 5294, 3454, 2008, 2031, 3047, 1010, 1996, 19220, 2008, 2003, 6230, 1010, 1996, 7922, 2108, 1996, 2088, 1521, 1055, 3914, 9598, 1010, 2045, 2003, 4415, 1037, 9874, 2000, 13299, 14200, 1998, 2009, 2003, 2183, 2000, 2272, 1012, 102]\n",
      "probs: tensor([[0.3757, 0.4670, 0.1574]], grad_fn=<SoftmaxBackward0>)\n",
      "pred ---  indices of the maximum value: 1\n",
      "input_ids_chunk: [101, 2153, 1010, 1045, 2123, 1521, 1056, 2113, 2043, 1998, 2129, 1010, 2021, 1996, 3037, 6165, 2031, 2042, 9561, 7406, 1998, 2027, 2323, 2022, 9561, 7406, 2582, 1012, 1524, 1001, 1001, 4803, 16189, 1037, 1520, 3671, 3444, 1521, 2174, 1010, 2025, 2035, 18288, 2024, 6427, 2008, 1996, 4125, 1999, 5416, 16189, 2003, 3430, 2005, 6089, 1012, 1999, 1037, 3602, 5958, 1010, 23724, 2015, 2132, 1997, 2647, 10067, 5656, 14459, 6187, 2226, 4081, 2008, 4803, 5416, 16189, 2020, 2058, 20041, 1010, 2004, 2027, 2018, 2042, 2474, 12588, 1996, 9229, 26632, 23035, 17680, 2005, 1996, 2117, 2431, 1997, 25682, 1010, 1998, 2056, 2027, 2020, 1037, 1523, 3671, 3444, 1524, 1997, 3171, 7233, 1012, 1523, 2007, 1996, 3145, 6853, 1997, 14200, 7302, 2039, 1010, 1996, 9824, 1997, 2130, 2062, 10807, 19220, 1999, 1996, 1057, 1012, 1055, 1012, 1998, 7279, 2102, 2039, 5157, 15801, 2011, 2152, 9987, 10995, 1010, 2009, 3849, 2157, 2005, 5416, 16189, 2000, 4608, 1011, 2039, 2007, 2060, 2062, 3935, 25416, 13490, 14279, 1010, 1524, 6187, 2226, 2056, 1010, 5815, 2008, 2430, 5085, 3961, 1523, 7933, 2006, 2907, 1524, 2445, 1996, 5703, 1997, 10831, 1012, 2002, 5275, 2008, 1996, 9561, 7406, 10750, 7774, 2003, 1523, 5171, 2012, 1996, 2220, 5711, 1997, 1996, 5402, 1010, 1524, 1998, 2008, 2061, 2146, 2004, 17404, 4897, 12166, 2024, 3144, 1010, 3930, 4247, 2000, 16356, 10745, 1998, 2430, 5085, 3961, 17145, 1010, 25416, 13490, 5649, 5829, 2408, 11412, 4280, 2298, 1523, 15123, 1524, 1998, 1041, 15549, 7368, 2323, 2022, 2583, 2000, 19319, 3020, 6165, 1012, 1523, 1997, 2607, 1010, 2044, 1996, 2844, 2693, 1997, 1996, 2197, 2261, 3134, 1010, 1041, 15549, 7368, 2071, 2928, 1037, 8724, 2004, 2116, 11105, 2008, 2031, 24356, 2007, 16189, 2298, 2058, 5092, 18533, 1010, 2066, 21955, 1998, 5085, 1010, 1524, 6187, 2226, 2056, 1012, 1523, 2021, 2012, 2023, 2754, 1010, 2057, 2228, 4803, 16189, 2024, 2062, 1037, 13964, 1997, 1996, 10067, 7087, 3006, 2084, 1037, 5081, 1010, 2061, 16510, 2015, 2323, 3613, 2000, 2022, 4149, 1012, 1524, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "probs: tensor([[0.7290, 0.2006, 0.0704]], grad_fn=<SoftmaxBackward0>)\n",
      "pred ---  indices of the maximum value: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1384, 0.8145, 0.0471]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3757, 0.4670, 0.1574]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.7290, 0.2006, 0.0704]], grad_fn=<SoftmaxBackward0>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# initialize probabilities list\n",
    "probs_list = []\n",
    "\n",
    "start = 0\n",
    "window_size = 510  # we take 2 off here so that we can fit in our [CLS] and [SEP] tokens\n",
    "\n",
    "loop = True\n",
    "\n",
    "while loop:\n",
    "    end = start + window_size\n",
    "    if end >= total_len:\n",
    "        loop = False\n",
    "        end = total_len\n",
    "    # (1) extract window from input_ids and attention_mask\n",
    "    input_ids_chunk = input_ids[start:end]\n",
    "    attention_mask_chunk = attention_mask[start:end]\n",
    "   \n",
    "    # (2) add [CLS] and [SEP]\n",
    "    input_ids_chunk = [101] + input_ids_chunk + [102]\n",
    "    attention_mask_chunk = [1] + attention_mask_chunk + [1]\n",
    "    \n",
    "    # (3) add padding upto window_size + 2 (512) tokens\n",
    "    # print(f\"window_size: {window_size}\" )\n",
    "    # print(f\"len(input_ids_chunk): {len(input_ids_chunk)}\" )\n",
    "    # print(f\"window_size - len(input_ids_chunk) + 2: {(window_size - len(input_ids_chunk) + 2)}\" )\n",
    "    input_ids_chunk += [0] * (window_size - len(input_ids_chunk) + 2)\n",
    "    print(f\"input_ids_chunk: {input_ids_chunk}\")\n",
    "    attention_mask_chunk += [0] * (window_size - len(attention_mask_chunk) + 2)\n",
    "    \n",
    "    # (4) format into PyTorch tensors dictionary\n",
    "    input_dict = {\n",
    "        'input_ids': torch.Tensor([input_ids_chunk]).long(),\n",
    "        'attention_mask': torch.Tensor([attention_mask_chunk]).int()\n",
    "    }\n",
    "    # (5) make logits prediction\n",
    "    outputs = model(**input_dict)\n",
    "    \n",
    "    # (6) calculate softmax and append to list\n",
    "    probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
    "    print(f\"probs: {probs}\")\n",
    "    pred = torch.argmax(probs)\n",
    "    print(f\"pred ---  indices of the maximum value: {pred}\")\n",
    "    probs_list.append(probs)\n",
    "\n",
    "    start = end\n",
    "    \n",
    "# let's view the probabilities given\n",
    "probs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each section has been assign varying levels of sentiment. The first and second sections both score *negatively* (index *1*) and the final sections scores *positively* (index *0*). To calculate the average sentiment across the full text, we will merge these tensors using the `stack` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1384, 0.8145, 0.0471]],\n",
       "\n",
       "        [[0.3757, 0.4670, 0.1574]],\n",
       "\n",
       "        [[0.7290, 0.2006, 0.0704]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacks = torch.stack(probs_list)\n",
    "stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we will calculate the mean score of each column (positive, negative, and neutral sentiment respectively) using `mean(dim=0)`. But before we do that we must reshape our tensor into a *3x3* shape - it is currently a 3x1x3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = stacks.shape\n",
    "shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape our tensor dimensions using the `resize_` method, and use dimensions `0` and `2` of our current tensor shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape[0], shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot resize variables that require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot resize variables that require grad"
     ]
    }
   ],
   "source": [
    "stacks.resize_(shape[0], shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try to resize our tensor, we will receive this `RuntimeError` telling us that we cannot resize variables that require *grad*. What this is referring to is the *gradient updates* of our model tensors during training. PyTorch cannot calculate gradients for tensors that have been reshaped. Fortunately, we don't actually want to use this tensor during any training, so we can use the `torch.no_grad()` namespace to tell PyTorch that we do **not** want to calculate any gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized stack: tensor([[0.1384, 0.8145, 0.0471],\n",
      "        [0.3757, 0.4670, 0.1574],\n",
      "        [0.7290, 0.2006, 0.0704]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.4144, 0.4940, 0.0916])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # we must include our stacks operation in here too\n",
    "    stacks = torch.stack(probs_list)\n",
    "    # now resize\n",
    "    stacks = stacks.resize_(stacks.shape[0], stacks.shape[2])\n",
    "    print(f\"Resized stack: {stacks}\")\n",
    "    # finally, we can calculate the mean value for each sentiment class\n",
    "    mean = stacks.mean(dim=0)\n",
    "    \n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final sentiment prediction shows a reasonable balanced sentiment of both positive and negative classes, with a slightly stronger negative sentiment score overall. We can take the `argmax` too to specify our winning class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(mean).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
