{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "TK add explanation of stopwords, + examples + some sample code + code we would actually use (eg NLTK)\n",
    "\n",
    "We will be using [this tweet](https://twitter.com/ivan_bezdomny/status/1367160747537682438) (don't worry, we will get to train some models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"\"\"I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\n",
    "\n",
    "Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.\\n\\nBoth impressed, and a little disappointed how rarely I get to actually train a model that matters :('"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the **NLTK** library for removing stopwords. NLTK comes with several stopword corpora, we will be using the English corpus. This corpus contains a huge number of English stopwords like *a*, *the*, *be*, *for*, *do*, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of stopwords. When we process our text data we will iterate through each word, if it is present in `stop_words` it will be removed. To optimize the speed of the stopword lookup we can convert `stop_words` to a `set` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to lowercase our text (because all of our stopwords are lowercased). Then we use split our input text into a list of tokens (each token is a word seperated by a space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i’m',\n",
       " 'amazed',\n",
       " 'how',\n",
       " 'often',\n",
       " 'in',\n",
       " 'practice,',\n",
       " 'not',\n",
       " 'only',\n",
       " 'does',\n",
       " 'a',\n",
       " '@huggingface',\n",
       " 'nlp',\n",
       " 'model',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problem,',\n",
       " 'but',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'public',\n",
       " 'finetuned',\n",
       " 'checkpoints,',\n",
       " 'is',\n",
       " 'good',\n",
       " 'enough',\n",
       " 'for',\n",
       " 'the',\n",
       " 'job.',\n",
       " 'both',\n",
       " 'impressed,',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'disappointed',\n",
       " 'how',\n",
       " 'rarely',\n",
       " 'i',\n",
       " 'get',\n",
       " 'to',\n",
       " 'actually',\n",
       " 'train',\n",
       " 'a',\n",
       " 'model',\n",
       " 'that',\n",
       " 'matters',\n",
       " ':(']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweet.lower().split()\n",
    "\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can iterate through the list, we check if each word exists in `stop_words` - if it does we discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With stopwords: i’m amazed how often in practice, not only does a @huggingface nlp model solve your problem, but one of their public finetuned checkpoints, is good enough for the job. both impressed, and a little disappointed how rarely i get to actually train a model that matters :(\n",
      "Without: i’m amazed often practice, @huggingface nlp model solve problem, one public finetuned checkpoints, good enough job. impressed, little disappointed rarely get actually train model matters :(\n"
     ]
    }
   ],
   "source": [
    "tweet_no_stopwords = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "print(\"With stopwords:\", ' '.join(tweet))\n",
    "print(\"Without:\", ' '.join(tweet_no_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that easy! We'll move onto more preprocessing methods in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### German Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Das Online-Geschäft hat drastisch zugenommen, entsprechend ist auch die Menge an Paketen gewachsen, die quer durch Deutschland geschickt werden. Die Paketboten kommen da schon einmal an ihre Grenzen. Meistens klappt die Zustellung durch DHL, Amazon oder Hermes reibungslos, doch es gibt unschöne Ausnahmen.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_de = \"Das Online-Geschäft hat drastisch zugenommen, entsprechend ist auch die Menge an Paketen gewachsen, die quer durch Deutschland geschickt werden. Die Paketboten kommen da schon einmal an ihre Grenzen. Meistens klappt die Zustellung durch DHL, Amazon oder Hermes reibungslos, doch es gibt unschöne Ausnahmen.\"\n",
    "sent_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber', 'alle', 'allem', 'allen', 'aller']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_de = stopwords.words('german')\n",
    "stop_words_de[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['das',\n",
       "  'online-geschäft',\n",
       "  'hat',\n",
       "  'drastisch',\n",
       "  'zugenommen,',\n",
       "  'entsprechend',\n",
       "  'ist',\n",
       "  'auch',\n",
       "  'die',\n",
       "  'menge'],\n",
       " 43)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_de = set(stop_words_de)\n",
    "sent_de_list = sent_de.lower().split()\n",
    "sent_de_list[:10], len(sent_de_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['online-geschäft',\n",
       "  'drastisch',\n",
       "  'zugenommen,',\n",
       "  'entsprechend',\n",
       "  'menge',\n",
       "  'paketen',\n",
       "  'gewachsen,',\n",
       "  'quer',\n",
       "  'deutschland',\n",
       "  'geschickt'],\n",
       " 25)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_de_no_stopwords = [word for word in sent_de_list if word not in stop_words_de]\n",
    "sent_de_no_stopwords[:10], len(sent_de_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: das online-geschäft hat drastisch zugenommen, entsprechend ist auch die menge an paketen gewachsen, die quer durch deutschland geschickt werden. die paketboten kommen da schon einmal an ihre grenzen. meistens klappt die zustellung durch dhl, amazon oder hermes reibungslos, doch es gibt unschöne ausnahmen.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {' '.join(sent_de_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: online-geschäft drastisch zugenommen, entsprechend menge paketen gewachsen, quer deutschland geschickt werden. paketboten kommen schon grenzen. meistens klappt zustellung dhl, amazon hermes reibungslos, gibt unschöne ausnahmen.\n"
     ]
    }
   ],
   "source": [
    "print(f\"After: {' '.join(sent_de_no_stopwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
