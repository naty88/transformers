{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is a *text normalization* method used in NLP to simplify text before it is processed by a model. When stemming break the final few characters of a word in order to find a common form of the word. If we take the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"I am amazed by how amazingly amazing you are\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use different forms of the word **amaze** a total of *three* times. Each of these different forms is called an *'inflection'*, which is the modification of a word to slightly adjust the meaning or context of the word. When we tokenize this text we produce three different tokens for each inflection of happy, which is okay but in many applications this level of granularity in the semantic meaning of the word is not required and can damage model performance.\n",
    "\n",
    "*Later, when we get to using more complex, sophisticated models (eg BERT), we will use different methods that maintain the inflection of each word - but it is important to understand stemming as it was a very important part of text preprocessing for a very long time, and still relevant to many applications.*\n",
    "\n",
    "To apply stemming we will be using the NLTK package, which provides several different stemmers, we will test the `PorterStemmer` and `LancasterStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('happi', 'happy'),\n",
       " ('happiest', 'happiest'),\n",
       " ('happier', 'happy'),\n",
       " ('cactu', 'cact'),\n",
       " ('cactii', 'cacti'),\n",
       " ('eleph', 'eleph'),\n",
       " ('eleph', 'eleph'),\n",
       " ('amaz', 'amaz'),\n",
       " ('amaz', 'amaz'),\n",
       " ('amazingli', 'amaz'),\n",
       " ('cement', 'cem'),\n",
       " ('owe', 'ow'),\n",
       " ('maximum', 'maxim')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_stem = ['happy', 'happiest', 'happier', 'cactus', 'cactii', 'elephant', 'elephants', 'amazed', 'amazing', 'amazingly', 'cement', 'owed', 'maximum']\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "stemmed = [(porter.stem(word), lancaster.stem(word)) for word in words_to_stem]\n",
    "stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter | Lancaster\n",
      "happi | happy\n",
      "happiest | happiest\n",
      "happier | happy\n",
      "cactu | cact\n",
      "cactii | cacti\n",
      "eleph | eleph\n",
      "eleph | eleph\n",
      "amaz | amaz\n",
      "amaz | amaz\n",
      "amazingli | amaz\n",
      "cement | cem\n",
      "owe | ow\n",
      "maximum | maxim\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter | Lancaster\")\n",
    "for stem in stemmed:\n",
    "    print(f\"{stem[0]} | {stem[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Porter stemmer](https://tartarus.org/martin/PorterStemmer/) is a set of rules that strip common suffixes from the ends of words, each of these rules are applied on after the other and produce our Porter stemmed words. It is a simple stemmer, and very fast.\n",
    "\n",
    "The [Lancaster stemmer](https://www.nltk.org/_modules/nltk/stem/lancaster.html) contains a larger set of rules and rather than applying each rule one after the other will keep iterating through the list of rules and find a rule that matches the current condition, which will then delete or replace the ending of the word. The iterations will stop once no more rules can be applied to the word OR if the word starts with a vowel and only two characters remain OR if the word starts with a consonant and there are three characters remaining. The Lancaster stemmer is much more aggressive in its stemming, sometimes this is a good thing, sometimes not.\n",
    "\n",
    "We can see from the results of the two stemmers above that neither are perfect, and this is the case with all stemming algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expert', 'expert'),\n",
       " ('expert', 'expert'),\n",
       " ('experten', 'expert'),\n",
       " ('expertin', 'expertin'),\n",
       " ('expertinnen', 'expertin'),\n",
       " ('gebäud', 'gebäud'),\n",
       " ('gebäud', 'gebäud'),\n",
       " ('gebäud', 'gebäud'),\n",
       " ('schön', 'schön'),\n",
       " ('schöner', 'schöner'),\n",
       " ('schönsten', 'schönsten')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_stem_de = [\"experte\", \"Experte\", \"Experten\", \"Expertin\", \"Expertinnen\", \"gebäude\", \"Gebäude\", \"Gebäudes\", \"schön\", \"schöner\", \"schönsten\"]\n",
    "stemmed_de = [(porter.stem(w), lancaster.stem(w)) for w in words_to_stem_de]\n",
    "stemmed_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy\n",
    "# ! python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_de = nlp(\" \".join(words_to_stem_de))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expert',\n",
       " 'Experte',\n",
       " 'Experte',\n",
       " 'Expertin',\n",
       " 'Expertin',\n",
       " 'gebäude',\n",
       " 'Gebäude',\n",
       " 'Gebäude',\n",
       " 'schön',\n",
       " 'schön',\n",
       " 'schönsen']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_lemma_de = [word.lemma_ for word in doc_de]\n",
    "spacy_lemma_de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with PyStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyStemmer in /home/nataliia/Workspaces/nlp_course/transformers/env/lib/python3.10/site-packages (2.2.0.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install PyStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ва'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "word = \"Вася\"\n",
    "word = stemmer.stemWord(word.lower())\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expert',\n",
       " 'expert',\n",
       " 'expert',\n",
       " 'expertin',\n",
       " 'expertinn',\n",
       " 'gebaud',\n",
       " 'gebaud',\n",
       " 'gebaud',\n",
       " 'schon',\n",
       " 'schon',\n",
       " 'schon']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer_de = Stemmer.Stemmer('german')\n",
    "stemmer_de_res = [stemmer_de.stemWord(w.lower()) for w in words_to_stem_de]\n",
    "stemmer_de_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experte: expert --- expert\n",
      "Experte: Experte --- expert\n",
      "Experten: Experte --- expert\n",
      "Expertin: Expertin --- expertin\n",
      "Expertinnen: Expertin --- expertinn\n",
      "gebäude: gebäude --- gebaud\n",
      "Gebäude: Gebäude --- gebaud\n",
      "Gebäudes: Gebäude --- gebaud\n",
      "schön: schön --- schon\n",
      "schöner: schön --- schon\n",
      "schönsten: schönsen --- schon\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.cistem import Cistem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exper',\n",
       " 'expert',\n",
       " 'expert',\n",
       " 'experti',\n",
       " 'expertinn',\n",
       " 'baud',\n",
       " 'baud',\n",
       " 'baud',\n",
       " 'schon',\n",
       " 'schoner',\n",
       " 'schon']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cistem = Cistem()\n",
    "cistem_res = [cistem.stem(word) for word in words_to_stem_de]\n",
    "cistem_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experte: expert --- expert --- exper\n",
      "Experte: Experte --- expert --- expert\n",
      "Experten: Experte --- expert --- expert\n",
      "Expertin: Expertin --- expertin --- experti\n",
      "Expertinnen: Expertin --- expertinn --- expertinn\n",
      "gebäude: gebäude --- gebaud --- baud\n",
      "Gebäude: Gebäude --- gebaud --- baud\n",
      "Gebäudes: Gebäude --- gebaud --- baud\n",
      "schön: schön --- schon --- schon\n",
      "schöner: schön --- schon --- schoner\n",
      "schönsten: schönsen --- schon --- schon\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(words_to_stem_de)):\n",
    "    print(f\"{words_to_stem_de[i]}: {spacy_lemma_de[i]} --- {stemmer_de_res[i]} --- {cistem_res[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
