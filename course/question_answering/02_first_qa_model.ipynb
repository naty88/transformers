{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514436d3",
   "metadata": {},
   "source": [
    "# First QA Model\n",
    "\n",
    "For our first QA model we will setup a simple question-answering pipeline using HuggingFace transformers and a pretrained BERT model. We will be testing it on our SQuAD data so let's load that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598be880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../data/squad/dev.json', 'r') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c964b0",
   "metadata": {},
   "source": [
    "As usual, we initialize our transformer tokenizer and model. This time, we will be using a BERT model that has been trained for question-and-answering on the SQuAD dataset. Which is why we will be using the validation dataset (rather than training dataset) from SQuAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6c9c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9731c8ae4dd14b579cfeb4227ceda7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0b987cd9a24d7199cbd0e28dc070b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875b606152b042a7a08c1fbcb43f4a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/152 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720507c9327d4eaa8c4dfa04cc3a267e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a731b3d02d6e4824b8f1756e6f30cf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "\n",
    "modelname = 'deepset/bert-base-cased-squad2'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(modelname)\n",
    "model = BertForQuestionAnswering.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ace0fb",
   "metadata": {},
   "source": [
    "Transformers comes with a useful class called [`pipeline`](https://huggingface.co/transformers/main_classes/pipelines.html) which allows us to setup easy to use pipelines for common architectures.\n",
    "\n",
    "One of those pipelines is the `question-answering` pipeline which allows us to feed a  dictionary containing a `'question'` and `'context'` and return an answer. Which we initialize like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfed29b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664c6328",
   "metadata": {},
   "source": [
    "Now we can begin asking questions, let's take a few examples from our `squad` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff6ad39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'In what country is Normandy located?',\n",
       "  'answer': 'France',\n",
       "  'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'},\n",
       " {'question': 'When were the Normans in Normandy?',\n",
       "  'answer': '10th and 11th centuries',\n",
       "  'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7a295f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9995271563529968, 'start': 159, 'end': 166, 'answer': 'France.'}\n",
      "{'score': 0.7168666124343872, 'start': 94, 'end': 117, 'answer': '10th and 11th centuries'}\n",
      "{'score': 0.7168666124343872, 'start': 94, 'end': 117, 'answer': '10th and 11th centuries'}\n",
      "{'score': 0.08210879564285278, 'start': 256, 'end': 283, 'answer': 'Denmark, Iceland and Norway'}\n",
      "{'score': 0.5019515156745911, 'start': 308, 'end': 314, 'answer': 'Rollo,'}\n"
     ]
    }
   ],
   "source": [
    "# we will intialize a list for answers\n",
    "answers = []\n",
    "\n",
    "for pair in squad[:5]:\n",
    "    # pass in our question and context to return an answer\n",
    "    ans = qa({\n",
    "        'question': pair['question'],\n",
    "        'context': pair['context']\n",
    "    })\n",
    "    print(ans)\n",
    "    # append predicted answer and real to answers list\n",
    "    answers.append({\n",
    "        'predicted': ans['answer'],\n",
    "        'true': pair['answer']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9f005e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'predicted': 'France.', 'true': 'France'},\n",
       " {'predicted': '10th and 11th centuries', 'true': '10th and 11th centuries'},\n",
       " {'predicted': '10th and 11th centuries',\n",
       "  'true': 'in the 10th and 11th centuries'},\n",
       " {'predicted': 'Denmark, Iceland and Norway',\n",
       "  'true': 'Denmark, Iceland and Norway'},\n",
       " {'predicted': 'Rollo,', 'true': 'Rollo'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512a524",
   "metadata": {},
   "source": [
    "So we can see that we're getting almost exact matches. Next, we'll take a look at how we can begin quantifying these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d44c53-00f5-4e32-8b70-c5bfef85ec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
