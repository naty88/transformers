{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Metrics\n",
    "\n",
    "We've put together our first Q&A model, and explored some of the metrics we can use to measure Q&A performance. In this notebook we're going to merge both of these and measure our Q&A model performance on the SQuAD 2.0 validation set as a whole.\n",
    "\n",
    "First, we load our SQuAD validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../data/squad/dev.json', 'r') as f:\n",
    "    squad = json.load(f)\n",
    "\n",
    "# we will limit it to the first 100 samples in the interest of time\n",
    "squad = squad[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's setup the QA pipeline again using the `deepset/bert-base-cased-squad2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, pipeline\n",
    "\n",
    "modelname = 'deepset/bert-base-cased-squad2'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(modelname)\n",
    "model = BertForQuestionAnswering.from_pretrained(modelname)\n",
    "\n",
    "qa = pipeline('question-answering', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we build a list of predicted answers `model_out` and true answers `reference` and calculate the ROUGE score based on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:57<00:00,  1.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_out = []\n",
    "reference = []\n",
    "\n",
    "for pair in tqdm(squad, leave=True):\n",
    "    ans = qa({\n",
    "        'question': pair['question'],\n",
    "        'context': pair['context']\n",
    "    })\n",
    "    # append the prediction and reference to the respective lists\n",
    "    model_out.append(ans['answer'])\n",
    "    reference.append(pair['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This make take some time to process. The processing speed of our models will improve as we begin using more efficient implementations over the next few sections.\n",
    "\n",
    "Once that has finished processing, we can calculate our ROUGE scores just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.550452380952381,\n",
       "  'p': 0.5426666666666666,\n",
       "  'f': 0.5268095208535719},\n",
       " 'rouge-2': {'r': 0.27141666666666664,\n",
       "  'p': 0.2693181818181818,\n",
       "  'f': 0.2580314670722219},\n",
       " 'rouge-l': {'r': 0.550452380952381,\n",
       "  'p': 0.5426666666666666,\n",
       "  'f': 0.5268095208535719}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# initialize\n",
    "rouge = Rouge()\n",
    "\n",
    "# get scores\n",
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem to be scoring as high as we would expect, if we print some of the results we can see why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollo,  |  Rollo  |  0.0\n",
      "north  |  the north  |  0.6666666622222223\n"
     ]
    }
   ],
   "source": [
    "# recalculate individual scores\n",
    "scores = rouge.get_scores(model_out, reference)\n",
    "\n",
    "print(model_out[4], ' | ', reference[4], ' | ', scores[4]['rouge-1']['f'])\n",
    "print(model_out[22], ' | ', reference[22], ' | ', scores[22]['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the punctuation differences are causing our ROUGE score to view these words as not matching. To fix this, we'll import `re` and remove any characters that are not spaces, letters, or numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "clean = re.compile('(?i)[^0-9a-z ]')\n",
    "\n",
    "# apply this to both lists\n",
    "model_out = [clean.sub('', text) for text in model_out]\n",
    "reference = [clean.sub('', text) for text in reference]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rollo  |  Rollo  |  0.999999995\n",
      "north  |  the north  |  0.6666666622222223\n"
     ]
    }
   ],
   "source": [
    "# recalculate individual scores\n",
    "scores = rouge.get_scores(model_out, reference)\n",
    "\n",
    "print(model_out[4], ' | ', reference[4], ' | ', scores[4]['rouge-1']['f'])\n",
    "print(model_out[22], ' | ', reference[22], ' | ', scores[22]['rouge-1']['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are looking better now, let's calculate the average again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.7429523809523809,\n",
       "  'p': 0.7693333333333334,\n",
       "  'f': 0.7236666627070026},\n",
       " 'rouge-2': {'r': 0.3114166666666667,\n",
       "  'p': 0.3093181818181818,\n",
       "  'f': 0.2980314669222219},\n",
       " 'rouge-l': {'r': 0.7429523809523809,\n",
       "  'p': 0.7693333333333334,\n",
       "  'f': 0.7236666627070026}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.get_scores(model_out, reference, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are seeing much more realistic scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
