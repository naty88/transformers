{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "naked-haiti",
   "metadata": {},
   "source": [
    "# Reader\n",
    "\n",
    "We've already explored the *reader* model in the first few sections of this chapter, where we loaded a pretrained model using the `transformers` library.\n",
    "\n",
    "Now, we'll do the same thing but via Haystack - which uses the same `transformers` library in the background (so we can use the same pretrained model names).\n",
    "\n",
    "To initialize our reader via Haystack all we need is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thrown-hanging",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "reader = FARMReader(model_name_or_path='deepset/bert-base-cased-squad2', use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-israeli",
   "metadata": {},
   "source": [
    "Now we have our reader model initialized, let's load up our FAISS index and DPR just like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advanced-outreach",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores.faiss import FAISSDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever\n",
    "\n",
    "path = '../../models/faiss'\n",
    "\n",
    "# load FAISS from file (the squad validation set index)\n",
    "document_store = FAISSDocumentStore.load(index_path=f'{path}/squad_dev.faiss')\n",
    "\n",
    "# initialize DPR model\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model='facebook/dpr-question_encoder-single-nq-base',\n",
    "    passage_embedding_model='facebook/dpr-ctx_encoder-single-nq-base',\n",
    "    use_gpu=True,\n",
    "    embed_title=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-habitat",
   "metadata": {},
   "source": [
    "Haystack comes with a very convenient `ExtractiveQAPipeline` class which allows us to pass our `reader` and `retriever` to build an easy-to-use *extractive* Q&A pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "contained-richards",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-brazil",
   "metadata": {},
   "source": [
    "We can ask questions using the `run` method, alongside the `query` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "australian-australian",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b95c23c3b746c4a6c0f02977592b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What does theoretical computer science cover?',\n",
       " 'no_ans_gap': 6.869088649749756,\n",
       " 'answers': [<Answer {'answer': 'analysis of algorithms and computability theory', 'type': 'extractive', 'score': 0.8070334196090698, 'context': ' related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms ', 'offsets_in_document': [{'start': 59, 'end': 106}], 'offsets_in_context': [{'start': 52, 'end': 99}], 'document_ids': ['42b3e25793765b085da634955c175f98'], 'meta': {'vector_id': '254'}}>,\n",
       "  <Answer {'answer': 'algorithmic problems', 'type': 'extractive', 'score': 0.4293513298034668, 'context': 'fore the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various resea', 'offsets_in_document': [{'start': 67, 'end': 87}], 'offsets_in_context': [{'start': 65, 'end': 85}], 'document_ids': ['27a839b0a3dba79e809de956f6832be6'], 'meta': {'vector_id': '114'}}>,\n",
       "  <Answer {'answer': 'Science of the Physical Universe', 'type': 'extractive', 'score': 0.3831794857978821, 'context': 'l Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. H', 'offsets_in_document': [{'start': 556, 'end': 588}], 'offsets_in_context': [{'start': 59, 'end': 91}], 'document_ids': ['fbccb09bae381f66ed2ec74583b5dc09'], 'meta': {'vector_id': '1182'}}>,\n",
       "  <Answer {'answer': 'computational complexity', 'type': 'extractive', 'score': 0.19299325346946716, 'context': 'Homer (2003) point out, the beginning of systematic studies in computational complexity is attributed to the seminal paper \"On the Computational Compl', 'offsets_in_document': [{'start': 76, 'end': 100}], 'offsets_in_context': [{'start': 63, 'end': 87}], 'document_ids': ['b8aadd8f533a6c82fee010c002294480'], 'meta': {'vector_id': '844'}}>,\n",
       "  <Answer {'answer': 'The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete', 'type': 'extractive', 'score': 0.1346004456281662, 'context': ' The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete', 'offsets_in_document': [{'start': 147, 'end': 354}], 'offsets_in_context': [{'start': 1, 'end': 208}], 'document_ids': ['5212bdd420adb42d8c7d7a4b38b39de8'], 'meta': {'vector_id': '334'}}>,\n",
       "  <Answer {'answer': 'parallel computing', 'type': 'extractive', 'score': 0.11263452470302582, 'context': 'used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to deter', 'offsets_in_document': [{'start': 536, 'end': 554}], 'offsets_in_context': [{'start': 66, 'end': 84}], 'document_ids': ['e7a93f38d4f2a6764c9c726d25ab6c99'], 'meta': {'vector_id': '1076'}}>,\n",
       "  <Answer {'answer': 'Complexity measures', 'type': 'extractive', 'score': 0.020939787849783897, 'context': 'any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other ', 'offsets_in_document': [{'start': 194, 'end': 213}], 'offsets_in_context': [{'start': 66, 'end': 85}], 'document_ids': ['40a9ac08f86dc4fa001efadd88a0dcc2'], 'meta': {'vector_id': '245'}}>,\n",
       "  <Answer {'answer': 'anything from an advanced supercomputer to a mathematician with a pencil and paper', 'type': 'extractive', 'score': 0.010902637615799904, 'context': ' representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem', 'offsets_in_document': [{'start': 293, 'end': 375}], 'offsets_in_context': [{'start': 34, 'end': 116}], 'document_ids': ['d2a8a5add2c09df16a63ed3c34c1d46'], 'meta': {'vector_id': '966'}}>,\n",
       "  <Answer {'answer': 'computational complexity theory, a problem refers to the abstract question to be solved', 'type': 'extractive', 'score': 0.0037602847442030907, 'context': 'sed with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of t', 'offsets_in_document': [{'start': 260, 'end': 347}], 'offsets_in_context': [{'start': 32, 'end': 119}], 'document_ids': ['6475dc896c098df79cbbe73c2f4b183d'], 'meta': {'vector_id': '430'}}>,\n",
       "  <Answer {'answer': 'classifying computational problems according to their inherent difficulty', 'type': 'extractive', 'score': 0.0003799298428930342, 'context': 'tical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each o', 'offsets_in_document': [{'start': 121, 'end': 194}], 'offsets_in_context': [{'start': 39, 'end': 112}], 'document_ids': ['ef80756dc751af204203f664fa56b1d4'], 'meta': {'vector_id': '1120'}}>],\n",
       " 'documents': [<Document: {'content': 'Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.', 'content_type': 'text', 'score': 0.6910437291292041, 'meta': {'vector_id': '1120'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'ef80756dc751af204203f664fa56b1d4'}>,\n",
       "  <Document: {'content': 'Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.', 'content_type': 'text', 'score': 0.6878912192671579, 'meta': {'vector_id': '254'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '42b3e25793765b085da634955c175f98'}>,\n",
       "  <Document: {'content': 'The four-year, full-time undergraduate program comprises a minority of enrollments at the university and emphasizes instruction with an \"arts and sciences focus\". Between 1978 and 2008, entering students were required to complete a core curriculum of seven classes outside of their concentration. Since 2008, undergraduate students have been required to complete courses in eight General Education categories: Aesthetic and Interpretive Understanding, Culture and Belief, Empirical and Mathematical Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. Harvard offers a comprehensive doctoral graduate program and there is a high level of coexistence between graduate and undergraduate degrees. The Carnegie Foundation for the Advancement of Teaching, The New York Times, and some students have criticized Harvard for its reliance on teaching fellows for some aspects of undergraduate education; they consider this to adversely affect the quality of education.', 'content_type': 'text', 'score': 0.6854423639363475, 'meta': {'vector_id': '1182'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'fbccb09bae381f66ed2ec74583b5dc09'}>,\n",
       "  <Document: {'content': 'A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.', 'content_type': 'text', 'score': 0.6822223436363057, 'meta': {'vector_id': '1076'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'e7a93f38d4f2a6764c9c726d25ab6c99'}>,\n",
       "  <Document: {'content': 'Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.', 'content_type': 'text', 'score': 0.682010360442883, 'meta': {'vector_id': '114'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '27a839b0a3dba79e809de956f6832be6'}>,\n",
       "  <Document: {'content': \"A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a thought experiment representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.\", 'content_type': 'text', 'score': 0.6815727723420425, 'meta': {'vector_id': '966'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'd2a8a5add2c09df16a63ed3c34c1d46'}>,\n",
       "  <Document: {'content': 'As Fortnow & Homer (2003) point out, the beginning of systematic studies in computational complexity is attributed to the seminal paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard Stearns (1965), which laid out the definitions of time and space complexity and proved the hierarchy theorems. Also, in 1965 Edmonds defined a \"good\" algorithm as one with running time bounded by a polynomial of the input size.', 'content_type': 'text', 'score': 0.6759214992600246, 'meta': {'vector_id': '844'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'b8aadd8f533a6c82fee010c002294480'}>,\n",
       "  <Document: {'content': 'In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.', 'content_type': 'text', 'score': 0.6755910092041354, 'meta': {'vector_id': '334'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '5212bdd420adb42d8c7d7a4b38b39de8'}>,\n",
       "  <Document: {'content': 'Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.', 'content_type': 'text', 'score': 0.6744374899103465, 'meta': {'vector_id': '245'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '40a9ac08f86dc4fa001efadd88a0dcc2'}>,\n",
       "  <Document: {'content': 'A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case \"no\"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.', 'content_type': 'text', 'score': 0.6741715630873673, 'meta': {'vector_id': '430'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '6475dc896c098df79cbbe73c2f4b183d'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {},\n",
       " 'node_id': 'Reader'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(query='What does theoretical computer science cover?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-breathing",
   "metadata": {},
   "source": [
    "Here we're returning a lot of different answers, which is not really necessary. We can limit the number of answers we return using two parameters, `top_k_retriever` and `top_k_reader`. Both of these limit the number of items being returned from the `retriever` and `reader` models respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "toxic-contrary",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b294c3fb20f48d8976dfe374d533055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What does theoretical computer science cover?',\n",
       " 'no_ans_gap': 6.869073867797852,\n",
       " 'answers': [<Answer {'answer': 'analysis of algorithms and computability theory', 'type': 'extractive', 'score': 0.8070324659347534, 'context': ' related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms ', 'offsets_in_document': [{'start': 59, 'end': 106}], 'offsets_in_context': [{'start': 52, 'end': 99}], 'document_ids': ['42b3e25793765b085da634955c175f98'], 'meta': {'vector_id': '254'}}>,\n",
       "  <Answer {'answer': 'algorithmic problems', 'type': 'extractive', 'score': 0.42935115098953247, 'context': 'fore the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various resea', 'offsets_in_document': [{'start': 67, 'end': 87}], 'offsets_in_context': [{'start': 65, 'end': 85}], 'document_ids': ['27a839b0a3dba79e809de956f6832be6'], 'meta': {'vector_id': '114'}}>,\n",
       "  <Answer {'answer': 'Science of the Physical Universe', 'type': 'extractive', 'score': 0.3831792175769806, 'context': 'l Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. H', 'offsets_in_document': [{'start': 556, 'end': 588}], 'offsets_in_context': [{'start': 59, 'end': 91}], 'document_ids': ['fbccb09bae381f66ed2ec74583b5dc09'], 'meta': {'vector_id': '1182'}}>,\n",
       "  <Answer {'answer': 'parallel computing', 'type': 'extractive', 'score': 0.11263357102870941, 'context': 'used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to deter', 'offsets_in_document': [{'start': 536, 'end': 554}], 'offsets_in_context': [{'start': 66, 'end': 84}], 'document_ids': ['e7a93f38d4f2a6764c9c726d25ab6c99'], 'meta': {'vector_id': '1076'}}>,\n",
       "  <Answer {'answer': 'classifying computational problems according to their inherent difficulty', 'type': 'extractive', 'score': 0.00037993222940713167, 'context': 'tical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each o', 'offsets_in_document': [{'start': 121, 'end': 194}], 'offsets_in_context': [{'start': 39, 'end': 112}], 'document_ids': ['ef80756dc751af204203f664fa56b1d4'], 'meta': {'vector_id': '1120'}}>],\n",
       " 'documents': [<Document: {'content': 'Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.', 'content_type': 'text', 'score': 0.6910437291292041, 'meta': {'vector_id': '1120'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'ef80756dc751af204203f664fa56b1d4'}>,\n",
       "  <Document: {'content': 'Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.', 'content_type': 'text', 'score': 0.6878912192671579, 'meta': {'vector_id': '254'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '42b3e25793765b085da634955c175f98'}>,\n",
       "  <Document: {'content': 'The four-year, full-time undergraduate program comprises a minority of enrollments at the university and emphasizes instruction with an \"arts and sciences focus\". Between 1978 and 2008, entering students were required to complete a core curriculum of seven classes outside of their concentration. Since 2008, undergraduate students have been required to complete courses in eight General Education categories: Aesthetic and Interpretive Understanding, Culture and Belief, Empirical and Mathematical Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. Harvard offers a comprehensive doctoral graduate program and there is a high level of coexistence between graduate and undergraduate degrees. The Carnegie Foundation for the Advancement of Teaching, The New York Times, and some students have criticized Harvard for its reliance on teaching fellows for some aspects of undergraduate education; they consider this to adversely affect the quality of education.', 'content_type': 'text', 'score': 0.6854423639363475, 'meta': {'vector_id': '1182'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'fbccb09bae381f66ed2ec74583b5dc09'}>,\n",
       "  <Document: {'content': 'A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.', 'content_type': 'text', 'score': 0.6822223436363057, 'meta': {'vector_id': '1076'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'e7a93f38d4f2a6764c9c726d25ab6c99'}>,\n",
       "  <Document: {'content': 'Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.', 'content_type': 'text', 'score': 0.682010360442883, 'meta': {'vector_id': '114'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '27a839b0a3dba79e809de956f6832be6'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Retriever': {'top_k': 5}},\n",
       " 'node_id': 'Reader'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(query='What does theoretical computer science cover?',\n",
    "             params={\"Retriever\": {\"top_k\": 5}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-philadelphia",
   "metadata": {},
   "source": [
    "And the `reader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "logical-barbados",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8738e3820a43eebbb861cd09985486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What does theoretical computer science cover?',\n",
       " 'no_ans_gap': 6.869088649749756,\n",
       " 'answers': [<Answer {'answer': 'analysis of algorithms and computability theory', 'type': 'extractive', 'score': 0.8070334196090698, 'context': ' related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms ', 'offsets_in_document': [{'start': 59, 'end': 106}], 'offsets_in_context': [{'start': 52, 'end': 99}], 'document_ids': ['42b3e25793765b085da634955c175f98'], 'meta': {'vector_id': '254'}}>,\n",
       "  <Answer {'answer': 'algorithmic problems', 'type': 'extractive', 'score': 0.4293513298034668, 'context': 'fore the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various resea', 'offsets_in_document': [{'start': 67, 'end': 87}], 'offsets_in_context': [{'start': 65, 'end': 85}], 'document_ids': ['27a839b0a3dba79e809de956f6832be6'], 'meta': {'vector_id': '114'}}>,\n",
       "  <Answer {'answer': 'Science of the Physical Universe', 'type': 'extractive', 'score': 0.3831794857978821, 'context': 'l Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. H', 'offsets_in_document': [{'start': 556, 'end': 588}], 'offsets_in_context': [{'start': 59, 'end': 91}], 'document_ids': ['fbccb09bae381f66ed2ec74583b5dc09'], 'meta': {'vector_id': '1182'}}>],\n",
       " 'documents': [<Document: {'content': 'Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.', 'content_type': 'text', 'score': 0.6910437291292041, 'meta': {'vector_id': '1120'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'ef80756dc751af204203f664fa56b1d4'}>,\n",
       "  <Document: {'content': 'Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, it tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kind of problems can, in principle, be solved algorithmically.', 'content_type': 'text', 'score': 0.6878912192671579, 'meta': {'vector_id': '254'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '42b3e25793765b085da634955c175f98'}>,\n",
       "  <Document: {'content': 'The four-year, full-time undergraduate program comprises a minority of enrollments at the university and emphasizes instruction with an \"arts and sciences focus\". Between 1978 and 2008, entering students were required to complete a core curriculum of seven classes outside of their concentration. Since 2008, undergraduate students have been required to complete courses in eight General Education categories: Aesthetic and Interpretive Understanding, Culture and Belief, Empirical and Mathematical Reasoning, Ethical Reasoning, Science of Living Systems, Science of the Physical Universe, Societies of the World, and United States in the World. Harvard offers a comprehensive doctoral graduate program and there is a high level of coexistence between graduate and undergraduate degrees. The Carnegie Foundation for the Advancement of Teaching, The New York Times, and some students have criticized Harvard for its reliance on teaching fellows for some aspects of undergraduate education; they consider this to adversely affect the quality of education.', 'content_type': 'text', 'score': 0.6854423639363475, 'meta': {'vector_id': '1182'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'fbccb09bae381f66ed2ec74583b5dc09'}>,\n",
       "  <Document: {'content': 'A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them, such as time and storage. Other complexity measures are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do.', 'content_type': 'text', 'score': 0.6822223436363057, 'meta': {'vector_id': '1076'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'e7a93f38d4f2a6764c9c726d25ab6c99'}>,\n",
       "  <Document: {'content': 'Before the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.', 'content_type': 'text', 'score': 0.682010360442883, 'meta': {'vector_id': '114'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '27a839b0a3dba79e809de956f6832be6'}>,\n",
       "  <Document: {'content': \"A Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a thought experiment representing a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway's Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.\", 'content_type': 'text', 'score': 0.6815727723420425, 'meta': {'vector_id': '966'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'd2a8a5add2c09df16a63ed3c34c1d46'}>,\n",
       "  <Document: {'content': 'As Fortnow & Homer (2003) point out, the beginning of systematic studies in computational complexity is attributed to the seminal paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard Stearns (1965), which laid out the definitions of time and space complexity and proved the hierarchy theorems. Also, in 1965 Edmonds defined a \"good\" algorithm as one with running time bounded by a polynomial of the input size.', 'content_type': 'text', 'score': 0.6759214992600246, 'meta': {'vector_id': '844'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': 'b8aadd8f533a6c82fee010c002294480'}>,\n",
       "  <Document: {'content': 'In 1967, Manuel Blum developed an axiomatic complexity theory based on his axioms and proved an important result, the so-called, speed-up theorem. The field really began to flourish in 1971 when the US researcher Stephen Cook and, working independently, Leonid Levin in the USSR, proved that there exist practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.', 'content_type': 'text', 'score': 0.6755910092041354, 'meta': {'vector_id': '334'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '5212bdd420adb42d8c7d7a4b38b39de8'}>,\n",
       "  <Document: {'content': 'Analogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.', 'content_type': 'text', 'score': 0.6744374899103465, 'meta': {'vector_id': '245'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '40a9ac08f86dc4fa001efadd88a0dcc2'}>,\n",
       "  <Document: {'content': 'A computational problem can be viewed as an infinite collection of instances together with a solution for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g. 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case \"no\"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.', 'content_type': 'text', 'score': 0.6741715630873673, 'meta': {'vector_id': '430'}, 'id_hash_keys': ['content'], 'embedding': '<embedding of shape (768,)>', 'id': '6475dc896c098df79cbbe73c2f4b183d'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Reader': {'top_k': 3}},\n",
       " 'node_id': 'Reader'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(query='What does theoretical computer science cover?',\n",
    "              params={\"Reader\": {\"top_k\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866a0e5-4624-4e25-a11d-cad490831723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
